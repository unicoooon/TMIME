{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import dot\n",
    "from keras.layers import MaxPool2D,Reshape\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Activation,Add\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg19 import VGG19\n",
    "from vgg16 import VGG16\n",
    "from resnet50 import ResNet50\n",
    "from resnetinception import InceptionResNetV2\n",
    "from densenet import DenseNet121\n",
    "from densenet import DenseNet169\n",
    "from densenet import DenseNet201\n",
    "from utils import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from types import FunctionType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple,List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "imgpath =[\"result_train/2014tr.png\",\n",
    "         \"result_train/2015tr.png\",\n",
    "         \"result_train/2016tr.png\",\n",
    "         \"result_train/2017tr.png\"]\n",
    "tifpath = [\"120041/LC81200412014210LGN00_merge_result.tif\",\n",
    "          \"120041/LC81200412015213LGN00_merge_result.tif\",\n",
    "          \"120041/LC81200412016232LGN00_merge_result.tif\",\n",
    "          \"120041/LC08_L1TP_120041_20170721_20170728_01_T1_merge_result.tif\"]\n",
    "imgSize = cv2.imread(imgpath[0],0)\n",
    "from keras.utils import np_utils\n",
    "def getData(maskpath:List,tifpath:List)->Tuple[ndarray,ndarray]:\n",
    "    '''\n",
    "    To get training data and training mask\n",
    "    args:\n",
    "        maskpath:the path of mask file\n",
    "        tifpath:the path of tif file\n",
    "    returns:\n",
    "        tmpData:trainng data with shape(1,imgSize.shape[0],imgsize.shape[1],7)\n",
    "        tmpMask:training mask with shape(1,imgSize.shape[0],imgsize.shape[1],2)\n",
    "    '''\n",
    "    assert isinstance(maskpath,list) and isinstance(tifpath,list)\n",
    "    tmpData = np.zeros((4,imgSize.shape[0],imgSize.shape[1],7))\n",
    "    tmpMask = np.zeros((4,imgSize.shape[0],imgSize.shape[1],2))\n",
    "    for i in range(4):\n",
    "        tmpData[i,:,:,:]=tifffile.imread(tifpath[i])\n",
    "        tmpMask[i,:,:,:] = np_utils.to_categorical(cv2.imread(maskpath[i],0),2)\n",
    "    return tmpData/255,tmpMask\n",
    "\n",
    "X_train,y_train = getData(imgpath,tifpath)\n",
    "\n",
    "img_cols=64*4\n",
    "img_rows=64*4\n",
    "num_channels=7\n",
    "num_mask_channels=2\n",
    "batch_size=2\n",
    "def dice_coef(y_true:ops.Tensor, y_pred:ops.Tensor)->ops.Tensor:\n",
    "    '''\n",
    "    To evacuate training model\n",
    "    args:\n",
    "        y_true:real mask\n",
    "        y_pred:predicted mask\n",
    "    returns:\n",
    "        :IOU of two mask\n",
    "    '''\n",
    "    assert isinstance(y_true,ops.Tensor) and isinstance(y_pred,ops.Tensor)\n",
    "    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)\n",
    "def flip_axis(x:ndarray, axis:int)->ndarray:\n",
    "    '''\n",
    "    To flip certain axis for given ndarray\n",
    "    args:\n",
    "        x:the ndarray we should flip\n",
    "        axis:which axis we should flip\n",
    "    returns:\n",
    "        :fliped ndarray\n",
    "    '''\n",
    "    assert isinstance(x,ndarray) and isinstance(axis,int)\n",
    "    x = np.asarray(x).swapaxes(axis, 0)\n",
    "    x = x[::-1, ...]\n",
    "    x = x.swapaxes(0, axis)\n",
    "    return x\n",
    "\n",
    "def form_batch(X:ndarray, y:ndarray, batch_size:int)->Tuple[ndarray,ndarray]:\n",
    "    '''\n",
    "    To get a batch training data\n",
    "    args:\n",
    "        X:whole training data\n",
    "        y:whole training mask\n",
    "        batch_size: training batch size\n",
    "    return:\n",
    "        X_batch:single batch with shape [batch_size,img_rows,img_cols,num_channels]\n",
    "        y_batch:single batch with shape [batch_size,img_rows,img_cols,num_mask_channels]\n",
    "    '''\n",
    "    assert isinstance(X,ndarray) and isinstance(y,ndarray) and isinstance(batch_size,int)\n",
    "    X_batch = np.zeros((batch_size, img_rows, img_cols,num_channels))\n",
    "    y_batch = np.zeros((batch_size, img_rows, img_cols,num_mask_channels))\n",
    "    X_height = X.shape[1]\n",
    "    X_width = X.shape[2]\n",
    "    for i in range(batch_size):\n",
    "        random_width = random.randint(0, X_width - img_cols - 1)\n",
    "        random_height = random.randint(0, X_height - img_rows - 1)\n",
    "\n",
    "        random_image = random.randint(0, X.shape[0] - 1)\n",
    "\n",
    "        y_batch[i] = y[random_image, random_height: random_height + img_rows, random_width: random_width + img_cols,:]\n",
    "        X_batch[i] = np.array(X[random_image,random_height: random_height + img_rows, random_width: random_width + img_cols,:])\n",
    "    return X_batch, y_batch\n",
    "\n",
    "def batch_generator(X:ndarray, y:ndarray, batch_size:int, horizontal_flip:bool=False, vertical_flip:bool=False, swap_axis:bool=False)->Tuple[ndarray,ndarray]:\n",
    "    '''\n",
    "    To get a batch training data\n",
    "    args:\n",
    "        X:whole training data\n",
    "        y:whole training mask\n",
    "        batch_size: training batch size\n",
    "        horizontal_flip:the way to flip data horizontally\n",
    "        vertical_flip:the way to flip data vertically\n",
    "        swap_axis:the way to swap axis\n",
    "    return:\n",
    "        X_batch:single batch with shape [batch_size,img_rows,img_cols,num_channels]\n",
    "        y_batch:single batch with shape [batch_size,img_rows,img_cols,num_mask_channels]\n",
    "    '''\n",
    "    assert isinstance(X,ndarray) and isinstance(y,ndarray) and isinstance(batch_size,int)\n",
    "    assert isinstance(horizontal_flip,bool) and isinstance(horizontal_flip,bool) and isinstance(horizontal_flip,bool)\n",
    "    while True:\n",
    "        X_batch, y_batch = form_batch(X, y, batch_size)\n",
    "\n",
    "        for i in range(X_batch.shape[0]):\n",
    "            xb = X_batch[i]\n",
    "            yb = y_batch[i]\n",
    "\n",
    "            if horizontal_flip:\n",
    "                if np.random.random() < 0.5:\n",
    "                    xb = flip_axis(xb, 1)\n",
    "                    yb = flip_axis(yb, 1)\n",
    "\n",
    "            if vertical_flip:\n",
    "                if np.random.random() < 0.5:\n",
    "                    xb = flip_axis(xb, 2)\n",
    "                    yb = flip_axis(yb, 2)\n",
    "\n",
    "            if swap_axis:\n",
    "                if np.random.random() < 0.5:\n",
    "                    xb = xb.swapaxes(1, 2)\n",
    "                    yb = yb.swapaxes(1, 2)\n",
    "            X_batch[i] = xb\n",
    "            y_batch[i] = yb\n",
    "        yield ([X_batch,X_batch,X_batch,X_batch,X_batch,X_batch,X_batch],y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RetModel()->Tuple[Model,Model,Model,Model,Model,Model,Model]:\n",
    "    '''\n",
    "    To get all training model\n",
    "    returns:\n",
    "        load vgg,resnet and densenet model\n",
    "    '''\n",
    "    return [VGG16(input_shape=(256,256,7),classes=2),VGG19(input_shape=(256,256,7),classes=2),\n",
    "           ResNet50(input_shape=(256, 256, 7), classes=2),InceptionResNetV2(input_shape=(256, 256, 7), classes=2),\n",
    "           DenseNet121(input_shape=(256, 256, 7),classes=2),DenseNet169(input_shape=(256, 256, 7), classes=2),\n",
    "           DenseNet201(input_shape=(256, 256, 7),classes=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SKIP_CONNECTIONS = {\n",
    "    'vgg16':            ('block5_conv3', 'block4_conv3', 'block3_conv3', 'block2_conv2', 'block1_conv2'),\n",
    "    'vgg19':            ('block5_conv4', 'block4_conv4', 'block3_conv4', 'block2_conv2', 'block1_conv2'), \n",
    "    'resnet50':         (141,79,37,4),\n",
    "    'inceptionresnetv2':    (606,266,16,9),\n",
    "    'densenet121':          (311, 139, 51, 4),\n",
    "    'densenet169':          (367, 139, 51, 4),\n",
    "    'densenet201':          (479, 139, 51, 4),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16net,vgg19net,resnet50,inceptionresnet,densenet121,densenet169,densenet201 = RetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "netList=[vgg16net,vgg19net,resnet50,inceptionresnet,densenet121,densenet169,densenet201]\n",
    "skip_con_List=[\"vgg\",\"vgg\",\"resnet\",\"inceptionresnet\",\"densenet\",\"densenet\",\"densenet\"]\n",
    "indexList=[18,21,173,779,425,593,705]\n",
    "backbone_name_List=[\"vgg16\",\"vgg19\",\"resnet50\",\"inceptionresnetv2\",\"densenet121\",\"densenet169\",\"densenet201\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_matmul(xylist:List)->ops.Tensor:\n",
    "    '''\n",
    "    To multiply two tensor\n",
    "    args:\n",
    "        xylist:list includes two tensor\n",
    "    return:\n",
    "        :result of two tensor's mat\n",
    "    '''\n",
    "    assert isinstance(xylist,List)\n",
    "    return K.dot(xylist[0],xylist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(channels:int)->FunctionType:\n",
    "    '''\n",
    "    Use keras stype to implemment attention layer \n",
    "    args:\n",
    "        channels:how many channels we should output\n",
    "    return:\n",
    "        a function implemmented self-attention\n",
    "    '''\n",
    "    assert isinstance(channels,int)\n",
    "    def self_attention(x:ops.Tensor)->ops.Tensor:\n",
    "        '''\n",
    "        To implemment self-attention\n",
    "        args:\n",
    "            x:input tensor for operation\n",
    "        return:\n",
    "            x:data after self-attention layers\n",
    "        '''\n",
    "        assert isinstance(x,ops.Tensor)\n",
    "        f = Conv2D(channels//8,kernel_size=1,strides=1,padding=\"same\")(x)\n",
    "        g = Conv2D(channels//8,kernel_size=1,strides=1,padding=\"same\")(x)\n",
    "        h = Conv2D(channels,kernel_size=1,strides=1,padding=\"same\")(x)\n",
    "        reg = Reshape((int(g.shape[-1]),int(g.shape[1])*int(g.shape[2])))(g)\n",
    "        regf = Reshape((int(f.shape[1])*int(f.shape[2]),int(f.shape[-1])))(f)\n",
    "        s = dot([reg,regf],axes=[1,2])\n",
    "        beta = Activation(\"softmax\")(s)\n",
    "        regh = Reshape(((int(h.shape[-1]),int(h.shape[1])*int(h.shape[2]))))(h)\n",
    "        o = dot([beta,regh],axes=[1,2])\n",
    "        o = Reshape((x.shape[1],x.shape[2],x.shape[3]))(x)\n",
    "        x = 0.2*o +x\n",
    "        return x\n",
    "    return self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_mean(x:ops.Tensor)->ops.Tensor:\n",
    "    '''\n",
    "    To get average of tensor\n",
    "    args:\n",
    "        x:tensor data\n",
    "    return:\n",
    "        :average of a tensor\n",
    "    '''\n",
    "    assert isinstance(x,ops.Tensor)\n",
    "    return K.mean(x,axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_max(x:ops.Tensor)->ops.Tensor:\n",
    "    '''\n",
    "    To get maximum of tensor\n",
    "    args:\n",
    "        x:tensor data\n",
    "    return:\n",
    "        :maximum of a tensor\n",
    "    '''\n",
    "    assert isinstance(x,ops.Tensor)\n",
    "    return K.max(x,axis=-1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(channels:int)->FunctionType:\n",
    "    '''\n",
    "    Use keras stype to implemment attention layer \n",
    "    args:\n",
    "        channels:how many channels we should output\n",
    "    return:\n",
    "        a function implemmented self-attention\n",
    "    '''\n",
    "    def channel_spatial_attention(x:ops.Tensor)->ops.Tensor:\n",
    "        '''\n",
    "        To implemment self-attention\n",
    "        args:\n",
    "            x:input tensor for operation\n",
    "        return:\n",
    "            x:data after self-attention layers\n",
    "        '''\n",
    "        xp = GlobalAveragePooling2D()(x)\n",
    "        xp = Dense(channels//10,activation='relu')(xp)\n",
    "        xp = Dense(channels)(xp)\n",
    "        xm = GlobalMaxPooling2D()(x)\n",
    "        xm = Dense(channels//10,activation='relu')(xm)\n",
    "        xm = Dense(channels)(xm)\n",
    "        ad = Add()([xp,xm])\n",
    "        scale = Reshape((1,1,channels))(ad)\n",
    "        scale = Activation(\"sigmoid\")(scale)\n",
    "        x = x*scale\n",
    "        x_avg = Lambda(keras_mean)(x)\n",
    "        x_max = Lambda(keras_max)(x)\n",
    "        scale = Concatenate(axis=-1)([x_avg,x_max])\n",
    "        scale = Conv2D(1,kernel_size=3,padding='same',strides=1,activation='sigmoid')(scale)\n",
    "        x = x*scale\n",
    "        return x\n",
    "    return channel_spatial_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(baseModel_list:List)->Tuple[List,ops.Tensor,List]:\n",
    "    '''\n",
    "    To get training model with attention layer and attention block,we use Lambda tranform tensorflow tensor to keras tensor\n",
    "    args:\n",
    "        baseModel_list:ensambling model list\n",
    "    return:\n",
    "        base_model_input:each base model input tensor\n",
    "        x:tensor after all keras layer\n",
    "    '''\n",
    "    assert isinstance(baseModel_list,List)\n",
    "    base_model_output = []\n",
    "    base_model_input = []\n",
    "    res_net_list=[]\n",
    "    for i in range(len(baseModel_list)):\n",
    "        model_name = backbone_name_List[i]+'_model_weight.h5'\n",
    "        net = Unet(netList[i],skip_index=DEFAULT_SKIP_CONNECTIONS,\n",
    "                   skip_con=skip_con_List[i],index=indexList[i],backbone_name=backbone_name_List[i])\n",
    "        net.load_weights(model_name)\n",
    "        res_net_list.append(net)\n",
    "        base_model_input.append(net.input)\n",
    "        base_model_output.append(net.output)\n",
    "    conc = Concatenate(axis=-1)(base_model_output)\n",
    "    x = Conv2D(64,kernel_size=3,strides=1,padding='same',activation='relu')(base_model_output[0])\n",
    "    x = Reshape((1,int(x.shape[1]),int(x.shape[2]),int(x.shape[3])))(x)\n",
    "    #x = ConvLSTM2D(64,kernel_size=3,padding='same',strides=1,return_sequences=False)(x)\n",
    "    x = Lambda(attention_layer(128))(x)\n",
    "    x = Conv2D(64,kernel_size=3,strides=1,padding='same',activation='relu')(x)\n",
    "    x = Reshape((1,int(x.shape[1]),int(x.shape[2]),int(x.shape[3])))(x)\n",
    "    #x = ConvLSTM2D(64,kernel_size=3,padding='same',strides=1,return_sequences=False)(x)\n",
    "    x = Lambda(attention_block(64))(x)\n",
    "    x = Conv2D(2,kernel_size=1,strides=1,padding='same',activation='softmax')(x)\n",
    "    return base_model_input,x,res_net_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_in_list,resX,res_list= model(netList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepModel = Model(base_in_list,resX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in res_list:\n",
    "    for layer in net.layers:\n",
    "        layer.trainable=False\n",
    "deepModel.compile(optimizer=Adam(),loss=\"categorical_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [ModelCheckpoint(\"attentionPlusSevenUnet.h5\",save_best_only=True,save_weights_only=True),\n",
    "                  ReduceLROnPlateau(monitor='val_loss',patience=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1219 - acc: 0.9538 - val_loss: 0.1047 - val_acc: 0.9533\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 29s 143ms/step - loss: 0.1066 - acc: 0.9575 - val_loss: 0.1272 - val_acc: 0.9504\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 29s 143ms/step - loss: 0.1312 - acc: 0.9482 - val_loss: 0.0930 - val_acc: 0.9531\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 28s 141ms/step - loss: 0.1112 - acc: 0.9568 - val_loss: 0.0559 - val_acc: 0.9794\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1242 - acc: 0.9516 - val_loss: 0.0606 - val_acc: 0.9815\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 28s 139ms/step - loss: 0.1188 - acc: 0.9547 - val_loss: 0.1020 - val_acc: 0.9548\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 28s 138ms/step - loss: 0.1296 - acc: 0.9501 - val_loss: 0.1884 - val_acc: 0.9240\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1191 - acc: 0.9531 - val_loss: 0.1222 - val_acc: 0.9598\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1118 - acc: 0.9564 - val_loss: 0.0568 - val_acc: 0.9791\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 28s 141ms/step - loss: 0.1059 - acc: 0.9593 - val_loss: 0.1277 - val_acc: 0.9475\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 28s 139ms/step - loss: 0.1271 - acc: 0.9513 - val_loss: 0.1239 - val_acc: 0.9552\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1125 - acc: 0.9560 - val_loss: 0.1348 - val_acc: 0.9454\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 28s 141ms/step - loss: 0.1131 - acc: 0.9558 - val_loss: 0.0424 - val_acc: 0.9839\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1083 - acc: 0.9571 - val_loss: 0.1309 - val_acc: 0.9559\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1234 - acc: 0.9517 - val_loss: 0.1745 - val_acc: 0.9294\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 28s 141ms/step - loss: 0.1109 - acc: 0.9573 - val_loss: 0.0711 - val_acc: 0.9778\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1172 - acc: 0.9542 - val_loss: 0.0485 - val_acc: 0.9815\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1096 - acc: 0.9559 - val_loss: 0.1626 - val_acc: 0.9304\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1102 - acc: 0.9570 - val_loss: 0.0818 - val_acc: 0.9739\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 28s 141ms/step - loss: 0.1265 - acc: 0.9513 - val_loss: 0.0975 - val_acc: 0.9632\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1079 - acc: 0.9576 - val_loss: 0.1587 - val_acc: 0.9331\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1116 - acc: 0.9560 - val_loss: 0.1362 - val_acc: 0.9439\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 28s 141ms/step - loss: 0.1141 - acc: 0.9543 - val_loss: 0.2019 - val_acc: 0.9157\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 28s 141ms/step - loss: 0.1182 - acc: 0.9540 - val_loss: 0.0377 - val_acc: 0.9866\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 28s 140ms/step - loss: 0.1002 - acc: 0.9613 - val_loss: 0.0934 - val_acc: 0.9615\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 28s 142ms/step - loss: 0.1184 - acc: 0.9533 - val_loss: 0.0571 - val_acc: 0.9765\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 29s 143ms/step - loss: 0.1013 - acc: 0.9609 - val_loss: 0.1264 - val_acc: 0.9457\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 29s 143ms/step - loss: 0.1173 - acc: 0.9542 - val_loss: 0.1105 - val_acc: 0.9568\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 28s 142ms/step - loss: 0.1126 - acc: 0.9560 - val_loss: 0.0415 - val_acc: 0.9836\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 29s 143ms/step - loss: 0.1112 - acc: 0.9562 - val_loss: 0.0381 - val_acc: 0.9852\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 29s 143ms/step - loss: 0.1102 - acc: 0.9562 - val_loss: 0.1713 - val_acc: 0.9272\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 29s 143ms/step - loss: 0.1127 - acc: 0.9550 - val_loss: 0.1248 - val_acc: 0.9502\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 29s 143ms/step - loss: 0.1094 - acc: 0.9571 - val_loss: 0.1567 - val_acc: 0.9419\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 29s 143ms/step - loss: 0.0992 - acc: 0.9606 - val_loss: 0.2036 - val_acc: 0.9167\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 29s 143ms/step - loss: 0.1055 - acc: 0.9577 - val_loss: 0.0714 - val_acc: 0.9730\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 28s 138ms/step - loss: 0.1025 - acc: 0.9597 - val_loss: 0.0915 - val_acc: 0.9671\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 0.1150 - acc: 0.9536 - val_loss: 0.1622 - val_acc: 0.9317\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 28s 138ms/step - loss: 0.1058 - acc: 0.9578 - val_loss: 0.0911 - val_acc: 0.9700\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 0.1060 - acc: 0.9578 - val_loss: 0.1262 - val_acc: 0.9560\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 28s 139ms/step - loss: 0.1084 - acc: 0.9570 - val_loss: 0.1540 - val_acc: 0.9365\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 27s 136ms/step - loss: 0.1004 - acc: 0.9599 - val_loss: 0.1365 - val_acc: 0.9306\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 0.1089 - acc: 0.9562 - val_loss: 0.0425 - val_acc: 0.9841\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 0.1198 - acc: 0.9531 - val_loss: 0.0771 - val_acc: 0.9712\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 0.1048 - acc: 0.9589 - val_loss: 0.1132 - val_acc: 0.9610\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 27s 137ms/step - loss: 0.1135 - acc: 0.9558 - val_loss: 0.1152 - val_acc: 0.9564\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 27s 136ms/step - loss: 0.0984 - acc: 0.9610 - val_loss: 0.0738 - val_acc: 0.9730\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 27s 136ms/step - loss: 0.1154 - acc: 0.9539 - val_loss: 0.1065 - val_acc: 0.9597\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 27s 135ms/step - loss: 0.1147 - acc: 0.9547 - val_loss: 0.1115 - val_acc: 0.9529\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 28s 139ms/step - loss: 0.1058 - acc: 0.9578 - val_loss: 0.1112 - val_acc: 0.9524\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 28s 139ms/step - loss: 0.1107 - acc: 0.9560 - val_loss: 0.1187 - val_acc: 0.9539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6dad9a92e8>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepModel.fit_generator(batch_generator(X_train,y_train,batch_size=batch_size),steps_per_epoch=200,epochs=50,\n",
    "                 validation_data=batch_generator(X_train,y_train,batch_size=batch_size),validation_steps=2,verbose=1,\n",
    "                 callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictData(modelt:Model,data_path:str)->ndarray:\n",
    "    '''\n",
    "    To predict data\n",
    "    args:\n",
    "        modelt:model we finished training\n",
    "        data_path:tif path we should predict\n",
    "    returns:\n",
    "        :predicted ndarray\n",
    "    '''\n",
    "    assert isinstance(modelt,Model) and isinstance(data_path,str)\n",
    "    x_img = tifffile.imread(data_path)/255\n",
    "    ocr = np.zeros((math.ceil(x_img.shape[0]/256)*256,math.ceil(x_img.shape[1]/256)*256,7),'float16')\n",
    "    ocr[0:x_img.shape[0],0:x_img.shape[1],:]=x_img\n",
    "    #ocr[x_img.shape[0]:,x_img.shape[0]:,:]=0\n",
    "    tmp = np.zeros((math.ceil(x_img.shape[0]/256)*256,math.ceil(x_img.shape[1]/256)*256))\n",
    "    for i in range(int(ocr.shape[0]/128)-1):\n",
    "        for j in range(int(ocr.shape[1]/128)-1):\n",
    "            pred = modelt.predict([np.expand_dims(ocr[128*i:128*(i+1)+128,128*j:128*(j+1)+128,:],0),\n",
    "                                   np.expand_dims(ocr[128*i:128*(i+1)+128,128*j:128*(j+1)+128,:],0),\n",
    "                                   np.expand_dims(ocr[128*i:128*(i+1)+128,128*j:128*(j+1)+128,:],0),\n",
    "                                   np.expand_dims(ocr[128*i:128*(i+1)+128,128*j:128*(j+1)+128,:],0),\n",
    "                                   np.expand_dims(ocr[128*i:128*(i+1)+128,128*j:128*(j+1)+128,:],0),\n",
    "                                   np.expand_dims(ocr[128*i:128*(i+1)+128,128*j:128*(j+1)+128,:],0),\n",
    "                                   np.expand_dims(ocr[128*i:128*(i+1)+128,128*j:128*(j+1)+128,:],0)])\n",
    "            pred = np.squeeze(pred)\n",
    "            tmp[128*i:128*(i+1)+128,128*j:128*(j+1)+128] = pred.argmax(axis=2)\n",
    "    rg =np.zeros((x_img.shape[0],x_img.shape[1]))\n",
    "    rg = tmp[0:x_img.shape[0],0:x_img.shape[1]]\n",
    "    tmpt = np.zeros((x_img.shape[0],x_img.shape[1],7))\n",
    "    for t in range(7):\n",
    "        tmpt[:,:,t]=rg\n",
    "    tmpt[x_img==0] = 0\n",
    "    return tmpt[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path =\"120041/LC81200412014210LGN00_merge_result.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predictData(deepModel,test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
